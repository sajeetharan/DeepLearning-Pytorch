{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fYF47Ed-9QGa"
   },
   "source": [
    "# Transformer: Attention is all you need\n",
    "\n",
    "This jupyter notebook is Tensorflow version implemented in the paper [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf). The task is translating a source human-readable datetime to a target fixed datetime format **yyyy-mm-dd**, e.g: \"24th Aug 19\" -> \"2019-08-24\". Best way to start implement a model from scratch is using small dataset and non-complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nS6uOMfi9QGd",
    "outputId": "083201ed-0468-419c-a770-ae16d25a3eeb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "from faker import Faker\n",
    "from babel.dates import format_date\n",
    "from nmt_utils import load_dataset_v2, preprocess_data, string_to_int, int_to_string, softmax\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DnNYOCLv9QGg",
    "outputId": "e3a3c23e-c0a9-4e09-9f0c-e5daefdd5f31",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m = 40000\n",
    "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset_v2(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3fpJKNw79QGi",
    "outputId": "43ce6fbb-9d70-41eb-ff52-46d0c70b6d94",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "human_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Sk1ilQZ9QGk",
    "outputId": "c6712468-1b8e-444d-e223-062e0a44ffb6"
   },
   "outputs": [],
   "source": [
    "machine_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IMBHN9CL9QGp",
    "outputId": "b71ed99b-d5d0-4d9a-c19f-2b5b9bcddb47",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Tx = 30\n",
    "Ty = 10\n",
    "\n",
    "X, Y = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty+1)\n",
    "\n",
    "print(\"X.shape:\", X.shape)\n",
    "print(\"Y.shape:\", Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZWIQ4QQC9QGr"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1K1xXXlm9QGt"
   },
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bVMN58Ba9QGv"
   },
   "outputs": [],
   "source": [
    "L = tf.keras.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vW7llLK69QGw"
   },
   "source": [
    "## Transformer model with Tensorflow.\n",
    "\n",
    "### Hyperparameter:\n",
    "\n",
    "$d_{model}$: dimension of word embeding, output of **Multi-head Attention** layer, output of **Feed Forward** layer.\n",
    "\n",
    "$d_k$: dimension of matrix Q, K\n",
    "\n",
    "$d_v$: dimension of matrix V\n",
    "\n",
    "$d_{ff}$: dimension of intermediate **Feed forward** layer\n",
    "\n",
    "$h$: number of heads at each block.\n",
    "\n",
    "\n",
    "### Positional Encoding:\n",
    "\n",
    "Since the Transformer model isn't sequential model like RNN and CNN. The computation is parallel over all input sentence flow from Embedding Layer, so we need to compute the relative or absolute position between the words. The author use non-trainable/fixed signusoid function:\n",
    "\n",
    "$$PE_{(pos, 2i)} = sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right) \\mbox{this corresponding to the even indices}$$\n",
    "$$PE_{(pos, 2i+1)} = cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right) \\mbox{this corresponding to the odd indices}$$\n",
    "\n",
    "where $pos$ is position in the sequence and $i$ is the dimension.\n",
    "\n",
    "\n",
    "### Scaled Dot-Product Attention:\n",
    "\n",
    "<img style=\"width:300px; height:300px\" src=\"https://i.imgur.com/HuXNlr0.png\" />\n",
    "\n",
    "$$Attention(Q, K, V) = softmax\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "### (Encoder-Decoder) Multi-Head Attention:\n",
    "\n",
    "<img style=\"weight:300px; height:300px\" src=\"https://i.imgur.com/vgfOLR2.png\" />\n",
    "\n",
    "$$MultiHead(Q, K, V) = Concat(head_1, head_2, ..., head_h)W^O$$\n",
    "$$\\mbox{where } head_i = Attention(Q, K, V)$$\n",
    "\n",
    "### Feed forward:\n",
    "\n",
    "$$FFN(x) = max(0, xW_1 + b_1)W_2 + b_2$$\n",
    "\n",
    "### Encoder blocks:\n",
    "\n",
    "Each encoder block include 2 layers: **Multi-head Attention Mechanism** and **Position-wise Feed Forward**, respestively. Output at each layer use residual connection with its input followed by [Layer Normalization](https://arxiv.org/pdf/1607.06450.pdf): $LayerNorm(x + f(x))$\n",
    "\n",
    "### Decoder blocks:\n",
    "\n",
    "Each decoder block includes 3 layers: **Multi-head Attention Mechanism**, **Encoder-Decoder Multi-head Attention** and **Position-wise Feed Forward**. Same as **Encoder** blocks, output at each layer use residual connection with its input follow by Layer Normalization.\n",
    "\n",
    "<img src=\"https://i.imgur.com/1NUHvLi.jpg\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DRbghOW09QGx"
   },
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, num_blocks, num_heads, vocab_size, seq_len, d_model, d_k, d_v, d_ff):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.num_blocks = num_blocks\n",
    "        self.num_heads = num_heads\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.d_ff = d_ff\n",
    "        self.word_embed = L.Embedding(vocab_size, d_model)\n",
    "        \n",
    "    def _format(self, block, head):\n",
    "        return str(block) + str(head)\n",
    "    \n",
    "    def _init_structure(self, decoder_part=False):\n",
    "        assert not hasattr(self, \"pos_enc\"), \"The structure is initialized already.\"\n",
    "        self.pos_enc = np.zeros(shape=(1, self.seq_len, self.d_model))\n",
    "        for pos in range(self.seq_len):\n",
    "            for i in range(0, self.d_model, 2):\n",
    "                self.pos_enc[:, pos, i] = np.sin(pos / (10000 ** ((2 * i)/self.d_model)))\n",
    "                self.pos_enc[:, pos, i + 1] = np.cos(pos / (10000 ** ((2 * i)/self.d_model)))\n",
    "        \n",
    "        if decoder_part:\n",
    "            self.mask = [[0]*(i+1) + [-1e9]*(self.seq_len-(i+1)) for i in range(self.seq_len)]\n",
    "            self.mask = np.array([self.mask])             \n",
    "        \n",
    "        for block_id in range(self.num_blocks):\n",
    "            setattr(self, \"Q\" + str(block_id), L.Dense(self.d_k*self.num_heads))\n",
    "            setattr(self, \"K\" + str(block_id), L.Dense(self.d_k*self.num_heads))\n",
    "            setattr(self, \"V\" + str(block_id), L.Dense(self.d_v*self.num_heads))\n",
    "            if decoder_part:\n",
    "                setattr(self, \"Qenc\" + str(block_id), L.Dense(self.d_k*self.num_heads))\n",
    "                setattr(self, \"Kenc\" + str(block_id), L.Dense(self.d_k*self.num_heads))\n",
    "                setattr(self, \"Venc\" + str(block_id), L.Dense(self.d_v*self.num_heads))\n",
    "            setattr(self, \"O\" + str(block_id), L.Dense(self.d_model))\n",
    "            setattr(self, \"FFN1\" + str(block_id), L.Dense(self.d_ff, activation=\"relu\"))\n",
    "            setattr(self, \"FFN2\" + str(block_id), L.Dense(self.d_model))\n",
    "            \n",
    "    def _ffn(self, block_id, attention_output):\n",
    "        ffn1 = getattr(self, \"FFN1\" + str(block_id))(attention_output)\n",
    "        ffn2 = getattr(self, \"FFN2\" + str(block_id))(ffn1)\n",
    "        return ffn2\n",
    "    \n",
    "    def _scaled_dot_product(self, Q, K, V, mask=False):\n",
    "        score = tf.matmul(Q, K, transpose_b=True)\n",
    "        if mask:\n",
    "            # apply mask to score, prevent the affect of feature words to current word.\n",
    "            score = score + self.mask[:, :score.shape[1], :score.shape[1]]\n",
    "        score = tf.nn.softmax(score/np.sqrt(self.d_k), axis=-1)\n",
    "        score = tf.matmul(score, V)\n",
    "        return score\n",
    "                \n",
    "    def _multi_head_attention(self, block_id, Q, K, V, connection_head=False, mask=False):\n",
    "        if connection_head:\n",
    "            Q = getattr(self, \"Qenc\" + str(block_id))(Q)\n",
    "            K = getattr(self, \"Kenc\" + str(block_id))(K)\n",
    "            V = getattr(self, \"Venc\" + str(block_id))(V)\n",
    "        else:\n",
    "            Q = getattr(self, \"Q\" + str(block_id))(Q)\n",
    "            K = getattr(self, \"K\" + str(block_id))(K)\n",
    "            V = getattr(self, \"V\" + str(block_id))(V)\n",
    "        score = self._scaled_dot_product(Q, K, V, mask)\n",
    "        head_output = getattr(self, \"O\" + str(block_id))(score)\n",
    "        return head_output\n",
    "    \n",
    "    def _block_computation(self, *args, **kwargs):\n",
    "        raise NotImplementedError(\"Transformer is abstract class. You must implement this function!\")\n",
    "        \n",
    "    def call(self, *args, **kwargs):\n",
    "        raise NotImplementedError(\"Transformer is abstract class. You must implement this function!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qjwvLJIt9QGz"
   },
   "outputs": [],
   "source": [
    "class Encoder(Transformer):\n",
    "    \n",
    "    def __init__(self, num_blocks, num_heads, vocab_size, seq_len, d_model, d_k, d_v, d_ff):\n",
    "        super(Encoder, self).__init__(num_blocks, num_heads, vocab_size, seq_len, d_model, d_k, d_v, d_ff)\n",
    "        self._init_structure()\n",
    "    \n",
    "    def _block_computation(self, block_id, x):\n",
    "        attention_output = self._multi_head_attention(block_id, x, x, x, connection_head=False, mask=False)\n",
    "        attention_output = L.LayerNormalization()(attention_output + x)\n",
    "        \n",
    "        block_output = self._ffn(block_id, attention_output)\n",
    "        block_output = L.LayerNormalization()(block_output + attention_output)\n",
    "        return block_output\n",
    "    \n",
    "    def call(self, x):\n",
    "        word_embed = self.word_embed(x)\n",
    "        word_embed = word_embed + self.pos_enc\n",
    "        \n",
    "        block_output = word_embed\n",
    "        for block_id in range(self.num_blocks):\n",
    "            block_output = self._block_computation(block_id, block_output)\n",
    "        return block_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A4wZMidU9QG0"
   },
   "outputs": [],
   "source": [
    "class Decoder(Transformer):\n",
    "    \n",
    "    def __init__(self, num_blocks, num_heads, vocab_size, seq_len, d_model, d_k, d_v, d_ff):\n",
    "        super(Decoder, self).__init__(num_blocks, num_heads, vocab_size, seq_len, d_model, d_k, d_v, d_ff)\n",
    "        self._init_structure(decoder_part=True)\n",
    "        self.logits = L.Dense(units=vocab_size)\n",
    "    \n",
    "    def _block_computation(self, block_id, x, encoder_output):\n",
    "        attention_output = self._multi_head_attention(block_id, x, x, x, connection_head=False, mask=True)\n",
    "        attention_output = L.LayerNormalization()(attention_output + x)\n",
    "        \n",
    "        connection_output = self._multi_head_attention(block_id, attention_output, encoder_output, \n",
    "                                                       encoder_output, connection_head=True, mask=False)\n",
    "        connection_output = L.LayerNormalization()(connection_output + attention_output)\n",
    "        \n",
    "        block_output = self._ffn(block_id, connection_output)\n",
    "        block_output = L.LayerNormalization()(block_output + connection_output)\n",
    "        return block_output\n",
    "    \n",
    "    def call(self, x, encoder_output):\n",
    "        word_embed = self.word_embed(x)\n",
    "        word_embed = word_embed + self.pos_enc[:, :word_embed.shape[1], :]\n",
    "        block_output = word_embed\n",
    "        for block_id in range(self.num_blocks):\n",
    "            block_output = self._block_computation(block_id, block_output, encoder_output)\n",
    "        logits = self.logits(block_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d-MDpWOz9QG2"
   },
   "outputs": [],
   "source": [
    "def loss_function(labels, logits):\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "    return tf.reduce_mean(tf.reduce_sum(loss, axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YrCRXy6r9QG6"
   },
   "source": [
    "### Define hyperparameter for Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "61n7LvL69QG6"
   },
   "outputs": [],
   "source": [
    "NUM_BLOCKS = 2\n",
    "NUM_HEADS = 2\n",
    "DIMENSION_MODEL = 32\n",
    "DIMENSION_K = 16\n",
    "DIMENSION_V = 16\n",
    "DIMENSION_FF = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gq4miYrG9QG9"
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(num_blocks=NUM_BLOCKS, num_heads=NUM_HEADS, vocab_size=len(human_vocab), seq_len=Tx, \n",
    "                  d_model=DIMENSION_MODEL, d_k=DIMENSION_K, d_v=DIMENSION_V, d_ff=DIMENSION_FF)\n",
    "\n",
    "decoder = Decoder(num_blocks=NUM_BLOCKS, num_heads=NUM_HEADS, vocab_size=len(machine_vocab), seq_len=Ty, \n",
    "                  d_model=DIMENSION_MODEL, d_k=DIMENSION_K, d_v=DIMENSION_V, d_ff=DIMENSION_FF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bWcjRibw9QG_"
   },
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "batch_size = 64\n",
    "num_batches = X.shape[0]//batch_size if X.shape[0] % batch_size == 0 else X.shape[0]//batch_size + 1\n",
    "data = tf.concat([X, Y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DW7SvQpg9QHA"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FAhJhuDA9QHC",
    "outputId": "8c2ab384-f58d-4719-f6aa-98e80a2acf51"
   },
   "outputs": [],
   "source": [
    "for e in range(epochs):\n",
    "    \n",
    "    data = tf.random.shuffle(data)\n",
    "    \n",
    "    X, Y = data[:, :Tx], data[:, Tx:]\n",
    "    \n",
    "    pbar = tqdm.tqdm_notebook(range(0, num_batches), desc=\"Epoch \" + str(e+1))\n",
    "    \n",
    "    train_loss = 0\n",
    "    \n",
    "    for it in pbar:\n",
    "        start = it*batch_size\n",
    "        end = (it+1)*batch_size\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            encoder_output = encoder(X[start:end])\n",
    "            \n",
    "            logits = decoder(Y[start:end, :-1], encoder_output)\n",
    "            print(logits.shape)\n",
    "            print(Y.shape)\n",
    "            loss = loss_function(Y[start:end, 1:], logits)\n",
    "        \n",
    "        train_loss += loss\n",
    "        \n",
    "        pbar.set_description(\"Epoch %s - Training loss: %f\" % (e+1, (train_loss / (it+1))))\n",
    "        \n",
    "        variables = encoder.variables + decoder.variables\n",
    "        \n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradients, variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "soUyqbWQ9QHD",
    "outputId": "e3993916-c079-4530-ba2b-0d89acf84d71",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\n",
    "\n",
    "for example in EXAMPLES:\n",
    "    source = string_to_int(example, Tx, human_vocab)\n",
    "    source = np.array([source])\n",
    "\n",
    "    encoder_output = encoder(source)\n",
    "    sentence = [machine_vocab[\"#\"]]\n",
    "\n",
    "    for t in range(Ty):\n",
    "        logits = decoder(np.array([sentence]), encoder_output)\n",
    "        prediction = tf.nn.softmax(logits, axis=-1)\n",
    "        prediction = np.argmax(prediction, axis=-1)\n",
    "        sentence.append(prediction[0][-1])\n",
    "\n",
    "    sequential_output = [inv_machine_vocab[s] for s in sentence[1:]]\n",
    "    parallel_output = [inv_machine_vocab[s] for s in prediction[0]]\n",
    "    \n",
    "    print(\"source:\", example)\n",
    "    print(\"sequential output:\", ''.join(sequential_output))\n",
    "    print(\"parallel output:\", ''.join(parallel_output))\n",
    "    print(\"-----------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Transformer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
